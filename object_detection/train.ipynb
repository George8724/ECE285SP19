{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'trainer' from 'object_detection' (G:\\project\\models-master\\research\\object_detection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0f6c7ccb7406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_rewriter_builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'trainer' from 'object_detection' (G:\\project\\models-master\\research\\object_detection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    " \n",
    "r\"\"\"Training executable for detection models.\n",
    "This executable is used to train DetectionModels. There are two ways of\n",
    "configuring the training job:\n",
    "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
    "can be specified by --pipeline_config_path.\n",
    "Example usage:\n",
    "    ./train \\\n",
    "        --logtostderr \\\n",
    "        --train_dir=path/to/train_dir \\\n",
    "        --pipeline_config_path=pipeline_config.pbtxt\n",
    "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
    "configuration file to define what type of DetectionModel is being trained, an\n",
    "input_reader_pb2.InputReader file to specify what training data will be used and\n",
    "a train_pb2.TrainConfig file to configure training parameters.\n",
    "Example usage:\n",
    "    ./train \\\n",
    "        --logtostderr \\\n",
    "        --train_dir=path/to/train_dir \\\n",
    "        --model_config_path=model_config.pbtxt \\\n",
    "        --train_config_path=train_config.pbtxt \\\n",
    "        --input_config_path=train_input_config.pbtxt\n",
    "\"\"\"\n",
    " \n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    " \n",
    "from object_detection import trainer\n",
    "from object_detection.builders import dataset_builder\n",
    "from object_detection.builders import graph_rewriter_builder\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import dataset_util\n",
    " \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    " \n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
    "flags.DEFINE_integer('task', 0, 'task id')\n",
    "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
    "flags.DEFINE_boolean('clone_on_cpu', False,\n",
    "                     'Force clones to be deployed on CPU.  Note that even if '\n",
    "                     'set to False (allowing ops to run on gpu), some ops may '\n",
    "                     'still be run on the CPU if they have no GPU kernel.')\n",
    "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
    "                     'replicas.')\n",
    "flags.DEFINE_integer('ps_tasks', 0,\n",
    "                     'Number of parameter server tasks. If None, does not use '\n",
    "                     'a parameter server.')\n",
    "flags.DEFINE_string('train_dir', '',\n",
    "                    'Directory to save the checkpoints and training summaries.')\n",
    " \n",
    "flags.DEFINE_string('pipeline_config_path', '',\n",
    "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
    "                    'file. If provided, other configs are ignored')\n",
    " \n",
    "flags.DEFINE_string('train_config_path', '',\n",
    "                    'Path to a train_pb2.TrainConfig config file.')\n",
    "flags.DEFINE_string('input_config_path', '',\n",
    "                    'Path to an input_reader_pb2.InputReader config file.')\n",
    "flags.DEFINE_string('model_config_path', '',\n",
    "                    'Path to a model_pb2.DetectionModel config file.')\n",
    " \n",
    "FLAGS = flags.FLAGS\n",
    " \n",
    " \n",
    "def main(_):\n",
    "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
    "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "  if FLAGS.pipeline_config_path:\n",
    "    configs = config_util.get_configs_from_pipeline_file(\n",
    "        FLAGS.pipeline_config_path)\n",
    "    if FLAGS.task == 0:\n",
    "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
    "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
    "                    overwrite=True)\n",
    "  else:\n",
    "    configs = config_util.get_configs_from_multiple_files(\n",
    "        model_config_path=FLAGS.model_config_path,\n",
    "        train_config_path=FLAGS.train_config_path,\n",
    "        train_input_config_path=FLAGS.input_config_path)\n",
    "    if FLAGS.task == 0:\n",
    "      for name, config in [('model.config', FLAGS.model_config_path),\n",
    "                           ('train.config', FLAGS.train_config_path),\n",
    "                           ('input.config', FLAGS.input_config_path)]:\n",
    "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
    "                      overwrite=True)\n",
    " \n",
    "  model_config = configs['model']\n",
    "  train_config = configs['train_config']\n",
    "  input_config = configs['train_input_config']\n",
    " \n",
    "  model_fn = functools.partial(\n",
    "      model_builder.build,\n",
    "      model_config=model_config,\n",
    "      is_training=True)\n",
    " \n",
    "  def get_next(config):\n",
    "    return dataset_util.make_initializable_iterator(\n",
    "        dataset_builder.build(config)).get_next()\n",
    " \n",
    "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
    " \n",
    "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "  cluster_data = env.get('cluster', None)\n",
    "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
    "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
    "  task_info = type('TaskSpec', (object,), task_data)\n",
    " \n",
    "  # Parameters for a single worker.\n",
    "  ps_tasks = 0\n",
    "  worker_replicas = 1\n",
    "  worker_job_name = 'lonely_worker'\n",
    "  task = 0\n",
    "  is_chief = True\n",
    "  master = ''\n",
    " \n",
    "  if cluster_data and 'worker' in cluster_data:\n",
    "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
    "    worker_replicas = len(cluster_data['worker']) + 1\n",
    "  if cluster_data and 'ps' in cluster_data:\n",
    "    ps_tasks = len(cluster_data['ps'])\n",
    " \n",
    "  if worker_replicas > 1 and ps_tasks < 1:\n",
    "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
    " \n",
    "  if worker_replicas >= 1 and ps_tasks > 0:\n",
    "    # Set up distributed training.\n",
    "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
    "                             job_name=task_info.type,\n",
    "                             task_index=task_info.index)\n",
    "    if task_info.type == 'ps':\n",
    "      server.join()\n",
    "      return\n",
    " \n",
    "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
    "    task = task_info.index\n",
    "    is_chief = (task_info.type == 'master')\n",
    "    master = server.target\n",
    " \n",
    "  graph_rewriter_fn = None\n",
    "  if 'graph_rewriter_config' in configs:\n",
    "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
    "        configs['graph_rewriter_config'], is_training=True)\n",
    " \n",
    "  trainer.train(\n",
    "      create_input_dict_fn,\n",
    "      model_fn,\n",
    "      train_config,\n",
    "      master,\n",
    "      task,\n",
    "      FLAGS.num_clones,\n",
    "      worker_replicas,\n",
    "      FLAGS.clone_on_cpu,\n",
    "      ps_tasks,\n",
    "      worker_job_name,\n",
    "      is_chief,\n",
    "      FLAGS.train_dir,\n",
    "      graph_hook_fn=graph_rewriter_fn)\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from object_detection.builders import dataset_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from object_detection.builders import graph_rewriter_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.legacy import trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
